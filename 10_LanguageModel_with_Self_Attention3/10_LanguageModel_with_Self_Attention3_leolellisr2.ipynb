{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aula 9 - Exercício - Leonardo de Lellis Rossi RA261900",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ea1722f983ca4dbab905d0b57a7fbebd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22caac4af23c4182bddf14cf2b465ff6",
              "IPY_MODEL_0528d88aec174a4795b42dce0de51243",
              "IPY_MODEL_4178545da8934431970a02e3e7fe3956"
            ],
            "layout": "IPY_MODEL_b5d8ccdadc8a4ee8abb8ef6ec21921d2"
          }
        },
        "22caac4af23c4182bddf14cf2b465ff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42f99a05a7024b5b8ade7eeb733bd46a",
            "placeholder": "​",
            "style": "IPY_MODEL_06b9e8386ed14a1b8c02a9bb6d162cc5",
            "value": "Downloading: 100%"
          }
        },
        "0528d88aec174a4795b42dce0de51243": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf80ed381bfb4d748d0e4884ee3dd865",
            "max": 209528,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23c30a3f936746d5ab8e191a6f94c0d0",
            "value": 209528
          }
        },
        "4178545da8934431970a02e3e7fe3956": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90078177eb4b42abb772650355c4e949",
            "placeholder": "​",
            "style": "IPY_MODEL_5f0becfe23ea4b2facd81e91c3f8bf94",
            "value": " 205k/205k [00:00&lt;00:00, 1.27MB/s]"
          }
        },
        "b5d8ccdadc8a4ee8abb8ef6ec21921d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42f99a05a7024b5b8ade7eeb733bd46a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06b9e8386ed14a1b8c02a9bb6d162cc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf80ed381bfb4d748d0e4884ee3dd865": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23c30a3f936746d5ab8e191a6f94c0d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "90078177eb4b42abb772650355c4e949": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f0becfe23ea4b2facd81e91c3f8bf94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fa7dafe13ee4854981cfc68d893e293": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_813eda626298430fbe5b50cb6dd942d3",
              "IPY_MODEL_b7926a46f5d44a289fdf91e18017513e",
              "IPY_MODEL_d603efdb5d184e31b102f95a80b4fa8f"
            ],
            "layout": "IPY_MODEL_adfb6f817b374f87a6d9859349a8b801"
          }
        },
        "813eda626298430fbe5b50cb6dd942d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21e0d1ea3743413aac180037d6a62650",
            "placeholder": "​",
            "style": "IPY_MODEL_6e86f27f3c684707a3ef667455420b95",
            "value": "Downloading: 100%"
          }
        },
        "b7926a46f5d44a289fdf91e18017513e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9ff1b342b1b4e0eb75683a36f654da8",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0276f4f9bfc04e9fa37bdcb119b5a54a",
            "value": 2
          }
        },
        "d603efdb5d184e31b102f95a80b4fa8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4b8985b70de4d3dab8e44f8fae0ff95",
            "placeholder": "​",
            "style": "IPY_MODEL_6cb923c8b31c40a48826be8f9fad8550",
            "value": " 2.00/2.00 [00:00&lt;00:00, 58.5B/s]"
          }
        },
        "adfb6f817b374f87a6d9859349a8b801": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21e0d1ea3743413aac180037d6a62650": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e86f27f3c684707a3ef667455420b95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9ff1b342b1b4e0eb75683a36f654da8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0276f4f9bfc04e9fa37bdcb119b5a54a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f4b8985b70de4d3dab8e44f8fae0ff95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cb923c8b31c40a48826be8f9fad8550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52409fd2a9fc482889674bc44a8c37b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7667e79779e4d1885c5e6a72ba066ff",
              "IPY_MODEL_253993bcb4a947219f844fef84eac075",
              "IPY_MODEL_cc172c860ef14a439f0ad908cc2e5a83"
            ],
            "layout": "IPY_MODEL_543b97e3c97e461899ade5e0252420b2"
          }
        },
        "b7667e79779e4d1885c5e6a72ba066ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c06951e5e8db4e7aa914f4c15c887b17",
            "placeholder": "​",
            "style": "IPY_MODEL_6e5c2bb31ce745dfa78260c686f277ef",
            "value": "Downloading: 100%"
          }
        },
        "253993bcb4a947219f844fef84eac075": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f439581dc7b4d35859aa2886d9bb084",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be1c29d4dbaa473d91d5b4598de6e71d",
            "value": 112
          }
        },
        "cc172c860ef14a439f0ad908cc2e5a83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f561edd0f90e4d6fbc4ed9f273b5d7aa",
            "placeholder": "​",
            "style": "IPY_MODEL_693c0a243dab439e8a1cf08503fb7a39",
            "value": " 112/112 [00:00&lt;00:00, 3.59kB/s]"
          }
        },
        "543b97e3c97e461899ade5e0252420b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c06951e5e8db4e7aa914f4c15c887b17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e5c2bb31ce745dfa78260c686f277ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f439581dc7b4d35859aa2886d9bb084": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be1c29d4dbaa473d91d5b4598de6e71d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f561edd0f90e4d6fbc4ed9f273b5d7aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "693c0a243dab439e8a1cf08503fb7a39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "122d292e01254337afb3e255de05aa55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3219e1a05a9542cfa975012fe5536dee",
              "IPY_MODEL_d6ca29644386404d86c9b532351385f0",
              "IPY_MODEL_4508e233b3974188bce39673e6948081"
            ],
            "layout": "IPY_MODEL_288fd5f7409349d090497b7581b82d47"
          }
        },
        "3219e1a05a9542cfa975012fe5536dee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a182e11369840dbbe174eb139028b10",
            "placeholder": "​",
            "style": "IPY_MODEL_8fd14b3d428d477f81a0cb876fde3844",
            "value": "Downloading: 100%"
          }
        },
        "d6ca29644386404d86c9b532351385f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d0b4eb9e52344c5b60c443e0af445b5",
            "max": 43,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d60362caffc54f4e90b9629f186c3540",
            "value": 43
          }
        },
        "4508e233b3974188bce39673e6948081": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fc847151d384d688a52da54cd85aa45",
            "placeholder": "​",
            "style": "IPY_MODEL_a0dd2dc1448c4c62803c1e9e8ccb17cd",
            "value": " 43.0/43.0 [00:00&lt;00:00, 641B/s]"
          }
        },
        "288fd5f7409349d090497b7581b82d47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a182e11369840dbbe174eb139028b10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fd14b3d428d477f81a0cb876fde3844": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d0b4eb9e52344c5b60c443e0af445b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d60362caffc54f4e90b9629f186c3540": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1fc847151d384d688a52da54cd85aa45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0dd2dc1448c4c62803c1e9e8ccb17cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24f4e6a1f4964bd49d36289b9e2e9321": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad14afc34d75480198cf7b86793a91e1",
              "IPY_MODEL_ddb88b1c87ba4843aae72e79f1c5d3d3",
              "IPY_MODEL_63431a78709d4f0c83b97a5063c71b61"
            ],
            "layout": "IPY_MODEL_aa877077c0ce44909544ef2993d3aff3"
          }
        },
        "ad14afc34d75480198cf7b86793a91e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2625c108afb947758282fba6473507a0",
            "placeholder": "​",
            "style": "IPY_MODEL_26290e631f594b299d50b81a02c17104",
            "value": "Downloading: 100%"
          }
        },
        "ddb88b1c87ba4843aae72e79f1c5d3d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a34db53f8f6549bdae83b1849664fced",
            "max": 647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d417f0630d63450e8d01f87102a7f850",
            "value": 647
          }
        },
        "63431a78709d4f0c83b97a5063c71b61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac5d0bfac42a496e800a771b0bc3364a",
            "placeholder": "​",
            "style": "IPY_MODEL_03d8de250c3a4c4c9a431b1645e8c0ca",
            "value": " 647/647 [00:00&lt;00:00, 5.61kB/s]"
          }
        },
        "aa877077c0ce44909544ef2993d3aff3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2625c108afb947758282fba6473507a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26290e631f594b299d50b81a02c17104": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a34db53f8f6549bdae83b1849664fced": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d417f0630d63450e8d01f87102a7f850": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ac5d0bfac42a496e800a771b0bc3364a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03d8de250c3a4c4c9a431b1645e8c0ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e70038d37e8f4b91b77c217996559890": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a7322167fdb44219a647d356ae22f66f",
              "IPY_MODEL_e81cb91ee032450fb4fbdc27a3154c95",
              "IPY_MODEL_94e77e09974e4f5e8366e6a2f85c5fa2"
            ],
            "layout": "IPY_MODEL_f84a231e0f5c42c68116e9c67aadde2d"
          }
        },
        "a7322167fdb44219a647d356ae22f66f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9006c11ff78d4d83b22624ede3c33e5d",
            "placeholder": "​",
            "style": "IPY_MODEL_b2560cf25d774579920790a9d1502c10",
            "value": "100%"
          }
        },
        "e81cb91ee032450fb4fbdc27a3154c95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_459aedb55cd74bd1b1b87b63dc0a0f98",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7caa31a7721c40199970eb8d2fed98d4",
            "value": 2
          }
        },
        "94e77e09974e4f5e8366e6a2f85c5fa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa13f767aaec47c8a090824a8417546e",
            "placeholder": "​",
            "style": "IPY_MODEL_5d566edd3e4b4c82909856eb20437e63",
            "value": " 2/2 [00:00&lt;00:00, 22.74it/s]"
          }
        },
        "f84a231e0f5c42c68116e9c67aadde2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9006c11ff78d4d83b22624ede3c33e5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2560cf25d774579920790a9d1502c10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "459aedb55cd74bd1b1b87b63dc0a0f98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7caa31a7721c40199970eb8d2fed98d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fa13f767aaec47c8a090824a8417546e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d566edd3e4b4c82909856eb20437e63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leolellisr/deep_learning_projects/blob/main/10_LanguageModel_with_Self_Attention3/10_LanguageModel_with_Self_Attention3_leolellisr2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nome = \"Leonardo de Lellis Rossi RA261900\"\n",
        "print(f'Meu nome é {nome}')\n",
        "\n",
        "last = '22/06/04_23h56'\n",
        "print(f'Last update: {last}')\n"
      ],
      "metadata": {
        "id": "jOdQB41_4ZxG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81bc844c-400c-4d51-9f11-95c3d30dc32d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome é Leonardo de Lellis Rossi RA261900\n",
            "Last update: 22/06/04_23h56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IbuChoAPMEn"
      },
      "source": [
        "#  Exercício - Aula 10: Refeito Modelo de Linguagem com auto-atenção"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_DBb0-Klwf2"
      },
      "source": [
        "Este exercício é similar ao da Aula 8, mas iremos agora treinar uma rede neural com **duas camadas** de auto-atenção **causais** para prever a próxima palavra de um texto, data as palavras anteriores como entrada. \n",
        "\n",
        "Iremos também trabalhar com sequencias de tamanho variável.\n",
        "\n",
        "Na camada de auto-atenção, não se esqueça de implementar:\n",
        "- Embeddings de posição\n",
        "- Projeções lineares (WQ, WK, WV, WO)\n",
        "- Conexões residuais\n",
        "- Camada de feed forward (2-layer MLP)\n",
        "\n",
        "\n",
        "O dataset usado neste exercício (BrWaC) possui um tamanho razoável e você vai precisar rodar seus experimentos com GPU.\n",
        "\n",
        "Alguns conselhos úteis:\n",
        "- **ATENÇÃO:** o dataset é bem grande. Não dê comando de imprimí-lo.\n",
        "- Durante a depuração, faça seu dataset ficar bem pequeno, para que a depuração seja mais rápida e não precise de GPU. Somente ligue a GPU quando o seu laço de treinamento já está funcionando\n",
        "- Não deixe para fazer esse exercício na véspera. Ele é trabalhoso."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iremos utilizar a biblioteca dos transformers para ter acesso ao tokenizador do BERT.\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3twP0YJC4jmJ",
        "outputId": "b84f73f7-10ee-4ee5-9ce3-125482d13e7b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 5.0 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 54.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 64.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnyhJZtTRNMx"
      },
      "source": [
        "## Importação dos pacotes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlIOVCajPWcU"
      },
      "source": [
        "import collections\n",
        "import itertools\n",
        "import functools\n",
        "import math\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm_notebook\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "debug = False\n",
        "checkinpoint = True\n",
        "save_in_drive = True\n",
        "\n",
        "params = {\n",
        "    'max_examples': 150_000_000,\n",
        "    'eval_every_steps': 10000,\n",
        "    'lr': 3e-4,\n",
        "    'batch_size': 128,\n",
        "    'embedding_dim': 128,\n",
        "    'hidden_size': 2*128,\n",
        "    'optimizer': 'Adam',\n",
        "    'retrain': True,\n",
        "    'path_saved_model': 'gdrive/MyDrive/Colab Notebooks/best_model_',\n",
        "    'path_saved_datasets': 'gdrive/MyDrive/Colab Notebooks/ds_',\n",
        "    'download_ds': False,\n",
        "    'aula': 'Aula10',\n",
        "    'max_seq_length': 9,\n",
        "    'train_examples': 90_000,\n",
        "    'valid_examples': 40_000,\n",
        "    'test_examples': 25_000,\n",
        "    'n_heads':4,\n",
        "    'last_step': 280000\n",
        "}\n",
        "params['path_saved_model'] = params['path_saved_model']+params['aula']+'_BS'+str(params['batch_size'])+'_HS'+str(params['hidden_size'])+'_EmbDim'+str(params['embedding_dim'])+'_MaxEx'+str(params['max_examples'])+'.pt'"
      ],
      "metadata": {
        "id": "dnfTE2XvSr3X"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which GPU we are using\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "w9f3PfifAwpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "def1c33d-b54d-4dc3-82c4-d5546e2df962"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jun  5 02:56:56 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "whTCe2i7AtoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seeds():\n",
        "  random.seed(123)\n",
        "  np.random.seed(123)\n",
        "  torch.manual_seed(123)\n",
        "  torch.cuda.manual_seed(123)\n",
        "set_seeds()"
      ],
      "metadata": {
        "id": "-O1Gy-aeUbv2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neptune config"
      ],
      "metadata": {
        "id": "9D9O0IdFUKdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install -U neptune-client\n",
        " import neptune.new as neptune"
      ],
      "metadata": {
        "id": "Ua-dPVPgUM3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a3cace3-f66b-4953-dee4-e0341094a58a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting neptune-client\n",
            "  Downloading neptune-client-0.16.3.tar.gz (317 kB)\n",
            "\u001b[K     |████████████████████████████████| 317 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting bravado\n",
            "  Downloading bravado-11.0.3-py2.py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 57.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (3.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.3.5)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Collecting PyJWT\n",
            "  Downloading PyJWT-2.4.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.3.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.15.0)\n",
            "Collecting websocket-client!=1.0.0,>=0.35.0\n",
            "  Downloading websocket_client-1.3.2-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.9 MB/s \n",
            "\u001b[?25hCollecting GitPython>=2.0.8\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 29.4 MB/s \n",
            "\u001b[?25hCollecting boto3>=1.16.0\n",
            "  Downloading boto3-1.24.2-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 69.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from neptune-client) (21.3)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.24.3)\n",
            "Collecting swagger-spec-validator>=2.7.4\n",
            "  Downloading swagger_spec_validator-2.7.4-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from neptune-client) (5.4.8)\n",
            "Collecting botocore<1.28.0,>=1.27.2\n",
            "  Downloading botocore-1.27.2-py3-none-any.whl (8.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 14.9 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.2 MB/s \n",
            "\u001b[?25hCollecting urllib3\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 70.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.2->boto3>=1.16.0->neptune-client) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=2.0.8->neptune-client) (4.2.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (3.0.4)\n",
            "Collecting urllib3\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 73.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from swagger-spec-validator>=2.7.4->neptune-client) (6.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from swagger-spec-validator>=2.7.4->neptune-client) (4.3.3)\n",
            "Collecting bravado-core>=5.16.1\n",
            "  Downloading bravado_core-5.17.0-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.8 MB/s \n",
            "\u001b[?25hCollecting monotonic\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting simplejson\n",
            "  Downloading simplejson-3.17.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (130 kB)\n",
            "\u001b[K     |████████████████████████████████| 130 kB 60.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (1.0.3)\n",
            "Collecting jsonref\n",
            "  Downloading jsonref-0.2-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (2022.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.18.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (21.4.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (5.7.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (4.11.4)\n",
            "Collecting webcolors>=1.11\n",
            "  Downloading webcolors-1.12-py3-none-any.whl (9.9 kB)\n",
            "Collecting uri-template\n",
            "  Downloading uri_template-1.2.0-py3-none-any.whl (10 kB)\n",
            "Collecting isoduration\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Collecting rfc3987\n",
            "  Downloading rfc3987-1.3.8-py2.py3-none-any.whl (13 kB)\n",
            "Collecting fqdn\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting rfc3339-validator\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Collecting jsonpointer>1.13\n",
            "  Downloading jsonpointer-2.3-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (3.8.0)\n",
            "Requirement already satisfied: cached-property>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from fqdn->jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.5.2)\n",
            "Collecting arrow>=0.15.0\n",
            "  Downloading arrow-1.2.2-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->neptune-client) (3.0.9)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas->neptune-client) (1.21.6)\n",
            "Building wheels for collected packages: neptune-client, future\n",
            "  Building wheel for neptune-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neptune-client: filename=neptune_client-0.16.3-py2.py3-none-any.whl size=570148 sha256=0f58fe20bf1061236e345135a55f17e81ea7dda8afa0b3838e34e0b76a80e51a\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/67/63/794a7079b23b633de6a77fb7e9427e368980a755c1bc52a814\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=f3853a25a177132fa0bcdbec5ad147acbac948d0bb156ea3b658f168a8abaaf5\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built neptune-client future\n",
            "Installing collected packages: arrow, webcolors, urllib3, uri-template, rfc3987, rfc3339-validator, jsonpointer, jmespath, isoduration, fqdn, swagger-spec-validator, smmap, simplejson, jsonref, botocore, s3transfer, monotonic, gitdb, bravado-core, websocket-client, PyJWT, GitPython, future, bravado, boto3, neptune-client\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.27 PyJWT-2.4.0 arrow-1.2.2 boto3-1.24.2 botocore-1.27.2 bravado-11.0.3 bravado-core-5.17.0 fqdn-1.5.1 future-0.18.2 gitdb-4.0.9 isoduration-20.11.0 jmespath-1.0.0 jsonpointer-2.3 jsonref-0.2 monotonic-1.6 neptune-client-0.16.3 rfc3339-validator-0.1.4 rfc3987-1.3.8 s3transfer-0.6.0 simplejson-3.17.6 smmap-5.0.0 swagger-spec-validator-2.7.4 uri-template-1.2.0 urllib3-1.25.11 webcolors-1.12 websocket-client-1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run = neptune.init(name= params['aula'], tags=[params['aula'], 'Auto-atenção', 'Self-Attention', 'checkinpoint', 'CrossEntropy', 'Adam', 'perplexity', 'BrWaC'],\n",
        "    project=\"leolellisr/dl-ia025\",\n",
        "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI1NjY1YmJkZi1hYmM5LTQ3M2QtOGU1ZC1iZTFlNWY4NjE1NDQifQ==\",\n",
        ")"
      ],
      "metadata": {
        "id": "KNvj9TrsUOdI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e194b69b-3e75-4b33-affc-8293741410b5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://app.neptune.ai/leolellisr/dl-ia025/e/DLIA-126\n",
            "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run['parameters'] = params"
      ],
      "metadata": {
        "id": "2Ld1XqRPVh21"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import itertools"
      ],
      "metadata": {
        "id": "K2517Ek9ZBv0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZfxgV2DUk58"
      },
      "source": [
        "## Implementação do MyDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_xhKm1EZ3bQ"
      },
      "source": [
        "from typing import List\n",
        "\n",
        "\n",
        "def tokenize(text: str, tokenizer):\n",
        "    return tokenizer(text, return_tensors=None, add_special_tokens=False).input_ids\n",
        "\n",
        "\n",
        "class MyDataset():\n",
        "    def __init__(self, texts: List[str], tokenizer, max_seq_length: int):\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.X = []\n",
        "        for text in tqdm_notebook(texts):\n",
        "          token_ids = tokenize(f'[CLS] {text}', tokenizer) # + [tokenizer.vocab['[SEP]']]  \n",
        "          token_ids += [tokenizer.vocab['[PAD]']] * max(0, 1 + max_seq_length - len(token_ids))\n",
        "          for i in range(0, len(token_ids) - 1, max_seq_length):\n",
        "            if i + max_seq_length < len(token_ids):\n",
        "              self.X.append(token_ids[i: i + max_seq_length + 1])\n",
        "            else:\n",
        "              self.X.append(token_ids[-max_seq_length - 1:])\n",
        "        self.X = torch.LongTensor(self.X)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Escreva seu código aqui\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Escreva seu código aqui\n",
        "        x_y_idx = self.X[idx]\n",
        "        return x_y_idx[:-1], x_y_idx[1:]       "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n"
      ],
      "metadata": {
        "id": "NOwJMEllqm4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "ea1722f983ca4dbab905d0b57a7fbebd",
            "22caac4af23c4182bddf14cf2b465ff6",
            "0528d88aec174a4795b42dce0de51243",
            "4178545da8934431970a02e3e7fe3956",
            "b5d8ccdadc8a4ee8abb8ef6ec21921d2",
            "42f99a05a7024b5b8ade7eeb733bd46a",
            "06b9e8386ed14a1b8c02a9bb6d162cc5",
            "bf80ed381bfb4d748d0e4884ee3dd865",
            "23c30a3f936746d5ab8e191a6f94c0d0",
            "90078177eb4b42abb772650355c4e949",
            "5f0becfe23ea4b2facd81e91c3f8bf94",
            "0fa7dafe13ee4854981cfc68d893e293",
            "813eda626298430fbe5b50cb6dd942d3",
            "b7926a46f5d44a289fdf91e18017513e",
            "d603efdb5d184e31b102f95a80b4fa8f",
            "adfb6f817b374f87a6d9859349a8b801",
            "21e0d1ea3743413aac180037d6a62650",
            "6e86f27f3c684707a3ef667455420b95",
            "f9ff1b342b1b4e0eb75683a36f654da8",
            "0276f4f9bfc04e9fa37bdcb119b5a54a",
            "f4b8985b70de4d3dab8e44f8fae0ff95",
            "6cb923c8b31c40a48826be8f9fad8550",
            "52409fd2a9fc482889674bc44a8c37b6",
            "b7667e79779e4d1885c5e6a72ba066ff",
            "253993bcb4a947219f844fef84eac075",
            "cc172c860ef14a439f0ad908cc2e5a83",
            "543b97e3c97e461899ade5e0252420b2",
            "c06951e5e8db4e7aa914f4c15c887b17",
            "6e5c2bb31ce745dfa78260c686f277ef",
            "6f439581dc7b4d35859aa2886d9bb084",
            "be1c29d4dbaa473d91d5b4598de6e71d",
            "f561edd0f90e4d6fbc4ed9f273b5d7aa",
            "693c0a243dab439e8a1cf08503fb7a39",
            "122d292e01254337afb3e255de05aa55",
            "3219e1a05a9542cfa975012fe5536dee",
            "d6ca29644386404d86c9b532351385f0",
            "4508e233b3974188bce39673e6948081",
            "288fd5f7409349d090497b7581b82d47",
            "2a182e11369840dbbe174eb139028b10",
            "8fd14b3d428d477f81a0cb876fde3844",
            "8d0b4eb9e52344c5b60c443e0af445b5",
            "d60362caffc54f4e90b9629f186c3540",
            "1fc847151d384d688a52da54cd85aa45",
            "a0dd2dc1448c4c62803c1e9e8ccb17cd",
            "24f4e6a1f4964bd49d36289b9e2e9321",
            "ad14afc34d75480198cf7b86793a91e1",
            "ddb88b1c87ba4843aae72e79f1c5d3d3",
            "63431a78709d4f0c83b97a5063c71b61",
            "aa877077c0ce44909544ef2993d3aff3",
            "2625c108afb947758282fba6473507a0",
            "26290e631f594b299d50b81a02c17104",
            "a34db53f8f6549bdae83b1849664fced",
            "d417f0630d63450e8d01f87102a7f850",
            "ac5d0bfac42a496e800a771b0bc3364a",
            "03d8de250c3a4c4c9a431b1645e8c0ca"
          ]
        },
        "outputId": "f0768c48-1bb8-4d32-cf83-eb6a10ef76be"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/205k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea1722f983ca4dbab905d0b57a7fbebd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0fa7dafe13ee4854981cfc68d893e293"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52409fd2a9fc482889674bc44a8c37b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/43.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "122d292e01254337afb3e255de05aa55"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/647 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24f4e6a1f4964bd49d36289b9e2e9321"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testando se a implementação do MyDataset está correta"
      ],
      "metadata": {
        "id": "wew-gFbWeBTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "debug = False\n",
        "params['batch_size'] = 10\n",
        "batch_size = params['batch_size'] \n",
        "dummy_texts = ['Eu gosto de correr', 'Ela gosta muito de comer pizza']\n",
        "\n",
        "dummy_dataset = MyDataset(texts=dummy_texts, tokenizer=tokenizer, max_seq_length=9)\n",
        "dummy_loader = DataLoader(dummy_dataset, batch_size=6, shuffle=False)\n",
        "print(f'len(dummy_dataset): {len(dummy_dataset)}')\n",
        "assert len(dummy_dataset) == 2\n",
        "print('Passou no assert de tamanho do dataset.')\n",
        "\n",
        "first_batch_input, first_batch_target = next(iter(dummy_loader))\n",
        "\n",
        "correct_first_batch_input = torch.LongTensor(\n",
        "    [[  101,  3396, 10303,   125, 13239,     0,     0,     0,     0],\n",
        "     [  101,  1660,  5971,   785,   125,  1847, 13779, 15616,     0]])\n",
        "\n",
        "correct_first_batch_target = torch.LongTensor(\n",
        "    [[ 3396, 10303,   125, 13239,     0,     0,     0,     0,     0],\n",
        "     [ 1660,  5971,   785,   125,  1847, 13779, 15616,     0,     0]])\n",
        "\n",
        "\n",
        "if debug: print(first_batch_input)\n",
        "\n",
        "assert torch.equal(first_batch_input, correct_first_batch_input)\n",
        "\n",
        "if debug:  print(first_batch_target)\n",
        "assert torch.equal(first_batch_target, correct_first_batch_target)\n",
        "\n",
        "print('Passou no assert de dataset.')"
      ],
      "metadata": {
        "id": "8r7jBFFUeApe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "e70038d37e8f4b91b77c217996559890",
            "a7322167fdb44219a647d356ae22f66f",
            "e81cb91ee032450fb4fbdc27a3154c95",
            "94e77e09974e4f5e8366e6a2f85c5fa2",
            "f84a231e0f5c42c68116e9c67aadde2d",
            "9006c11ff78d4d83b22624ede3c33e5d",
            "b2560cf25d774579920790a9d1502c10",
            "459aedb55cd74bd1b1b87b63dc0a0f98",
            "7caa31a7721c40199970eb8d2fed98d4",
            "fa13f767aaec47c8a090824a8417546e",
            "5d566edd3e4b4c82909856eb20437e63"
          ]
        },
        "outputId": "63c3b504-d2dc-4b9f-ca68-0f34ef68e188"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  if sys.path[0] == '':\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e70038d37e8f4b91b77c217996559890"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(dummy_dataset): 2\n",
            "Passou no assert de tamanho do dataset.\n",
            "Passou no assert de dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "if save_in_drive: drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIwYycaugsxY",
        "outputId": "f58fc515-9b21-43ef-c0ad-f425c68e0216"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LfrHHouleJ0"
      },
      "source": [
        "# Carregamento do dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2vFWjsSkmop"
      },
      "source": [
        "Iremos usar uma pequena amostra do dataset [BrWaC](https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC) para treinar e avaliar nosso modelo de linguagem."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula9/sample-1gb.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGlN1WqrXPA6",
        "outputId": "13446e59-7bf8-492f-c5c0-a4ce23295500"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-05 02:57:51--  https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula9/sample-1gb.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.69.128, 64.233.191.128, 173.194.192.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.69.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1230909256 (1.1G) [text/plain]\n",
            "Saving to: ‘sample-1gb.txt’\n",
            "\n",
            "sample-1gb.txt      100%[===================>]   1.15G   137MB/s    in 8.3s    \n",
            "\n",
            "2022-06-05 02:58:00 (142 MB/s) - ‘sample-1gb.txt’ saved [1230909256/1230909256]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "max_seq_length = params['max_seq_length']\n",
        "\n",
        "train_examples = params['train_examples']\n",
        "valid_examples = params['valid_examples']\n",
        "test_examples = params['test_examples']\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "gxa_4gmiA-wE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NGpsakPdG4Nw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_lines = train_examples + valid_examples + test_examples\n",
        "print(f'Truncating to {max_lines} lines.')\n",
        "\n",
        "if params['download_ds']:\n",
        "  texts = open('sample-1gb.txt').readlines()\n",
        "  print(f'Read {len(texts)} lines.')\n",
        "  \n",
        "  #smart batching\n",
        "  texts = sorted(texts, key=lambda x: len(x[0]))\n",
        "\n",
        "  texts = texts[:max_lines]  \n",
        "  \n",
        "  training_texts = texts[:-(valid_examples + test_examples)]\n",
        "  valid_texts = texts[-(valid_examples + test_examples):-test_examples]\n",
        "  test_texts = texts[-test_examples:]\n",
        "\n",
        "  training_dataset = MyDataset(texts=training_texts, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "  valid_dataset = MyDataset(texts=valid_texts, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "  test_dataset = MyDataset(texts=test_texts, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "else:\n",
        "  path = params['path_saved_datasets']+'train.pt'\n",
        "  training_dataset = torch.load(path)\n",
        "  path = params['path_saved_datasets']+'val.pt'\n",
        "  valid_dataset = torch.load(path)\n",
        "  path = params['path_saved_datasets']+'test.pt'\n",
        "  test_dataset = torch.load(path)\n",
        "params['batch_size'] = 64\n",
        "batch_size = params['batch_size'] \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDari2UL78_5",
        "outputId": "a9f29955-250d-4c0f-b075-bcf8f3ed7e52"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Truncating to 155000 lines.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'training examples: {len(training_dataset)}')\n",
        "print(f'valid examples: {len(valid_dataset)}')\n",
        "print(f'test examples: {len(test_dataset)}')"
      ],
      "metadata": {
        "id": "KCSGJ5m7py4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58d19afe-f323-4117-bf5e-029deeeb9368"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training examples: 11198040\n",
            "valid examples: 5011771\n",
            "test examples: 3041508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if save_in_drive and params['download_ds']:\n",
        "  torch.save(training_dataset, params['path_saved_datasets']+\"train.pt\")\n",
        "  torch.save(valid_dataset, params['path_saved_datasets']+\"val.pt\")\n",
        "  torch.save(test_dataset, params['path_saved_datasets']+\"test.pt\")"
      ],
      "metadata": {
        "id": "VeNzN6ZjWP09"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "CmjYdmqANlRn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# É recomendado reiniciar as seeds antes de inicializar o modelo, pois assim\n",
        "# garantimos que os pesos vao ser sempre os mesmos.\n",
        "set_seeds()\n",
        "\n",
        "class SelfAttentionLayer(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, padding_idx, n_heads, dim, max_length):\n",
        "        super().__init__()\n",
        "        # n_heads: H\n",
        "        # dim: D\n",
        "        # max_lenght: L\n",
        "        # vocab_lenght: V\n",
        "        self.H = n_heads\n",
        "        self.D = dim\n",
        "        self.L = max_length\n",
        "        self.D_H = self.D // self.H # D / H\n",
        "        self.pad = padding_idx\n",
        "\n",
        "        self.W_q = torch.nn.Linear(self.D, self.D, bias=False) # (D, D)\n",
        "        self.W_k = torch.nn.Linear(self.D, self.D, bias=False)\n",
        "        self.W_v = torch.nn.Linear(self.D, self.D, bias=False)\n",
        "        self.W_o = torch.nn.Linear(self.D, self.D, bias=False)\n",
        "\n",
        "        self.layer_norm1  = torch.nn.LayerNorm(self.D, eps=1e-6)\n",
        "\n",
        "        self.feed_forward = torch.nn.Sequential(\n",
        "            torch.nn.Linear(self.D, self.D*10),  \n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(self.D*10, self.D)\n",
        "        )\n",
        "        self.layer_norm2  = torch.nn.LayerNorm(self.D, eps=1e-6)\n",
        "\n",
        "\n",
        "    def forward(self, x, att_mask):\n",
        "        # multi-head self-attention\n",
        "        \n",
        "        fQ = self.W_q(x).reshape(len(x), self.L, self.H, self.D_H) # (B, L, H, D/H)\n",
        "        fK = self.W_k(x).reshape(len(x), self.L, self.H, self.D_H)\n",
        "        fV = self.W_v(x).reshape(len(x), self.L, self.H, self.D_H)\n",
        "\n",
        "        # (B, L, H, D/H) -> (B, H, L, D/H)\n",
        "        fQ_transposed = fQ.transpose(1, 2)                    # (B, H, L, D/H)\n",
        "        fK_transposed = fK.transpose(1, 2)\n",
        "        fV_transposed = fV.transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(fQ_transposed, fK_transposed.transpose(-2, -1)) / math.sqrt(self.D_H) # (B, H, L, L)\n",
        "\n",
        "        scores = scores.masked_fill(att_mask.unsqueeze(1) == self.pad, -float(\"inf\"))\n",
        "\n",
        "        if debug: print(f\"scores (B, H, L, L): {scores.shape}\")  \n",
        "\n",
        "        probs = F.softmax(scores, dim=-1) # shape = B, L, L\n",
        "        if debug: print(f\"probs (B, H, L, L): {probs.shape}\")  \n",
        "\n",
        "        E = torch.matmul(probs, fV_transposed)\n",
        "        if debug: print(f\"E (B, H, L, D/H): {E.shape}\")  \n",
        "\n",
        "        out = E.transpose(1, 2).contiguous()                   # (B, L, H, D/H)\n",
        "        out = out.reshape(len(x), self.L, self.D) # (B, L, D) \n",
        "        out = self.W_o(out)\n",
        "        if debug: print(f\"out (B, L, D): {out.shape}\")  # (B, L, D)    \n",
        "\n",
        "        out = self.layer_norm1(x+out)               # (B, L, D) \n",
        "        out = self.feed_forward(out)\n",
        "        out = self.layer_norm2(x+out)           # (B, L, D) \n",
        "        #att_mean = att_norm2 * padMask.unsqueeze(-1)      \n",
        "        #mean_embeddings = att_mean.sum(dim=1) / padMask.count_nonzero(-1).unsqueeze(1)  \n",
        "        return out"
      ],
      "metadata": {
        "id": "GgdE_7xnMp7Y"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MsAs84KmQVXc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGaAjYDfWdd1"
      },
      "source": [
        "set_seeds()\n",
        "class LanguageModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size: int, max_seq_length: int, dim: int, n_layers: int, pad_token_id: int, hidden: int, n_heads):\n",
        "        \"\"\"\n",
        "        Implements the Self-attention, decoder-only.\"\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the input vocabulary.\n",
        "            max_seq_length (int): Size of the sequence to consider as context for prediction.\n",
        "            dim (int): Dimension of the embedding layer for each word in the context.\n",
        "            n_layers (int): number of self-attention layers.\n",
        "            pad_token_id (int): id of the pad token that will be ignored in the attention.\n",
        "          \n",
        "        \"\"\"\n",
        "        # Escreva seu código aqui.\n",
        "        super().__init__()\n",
        "        self.H = n_heads \n",
        "        self.D = dim\n",
        "        self.L = max_seq_length\n",
        "        self.D_H = self.D // self.H # D / H\n",
        "        self.pad_token_id = pad_token_id\n",
        "        self.V = vocab_size\n",
        "        self.hidden_size = hidden\n",
        "        # word embedding \n",
        "        self.embeddings_c = nn.Embedding(self.V, self.D,padding_idx=pad_token_id)\n",
        "        self.embeddings_p = torch.nn.Linear(self.D, self.L, bias=False)\n",
        "\n",
        "#        self.att_layer = \n",
        "        self.att_layer1 = SelfAttentionLayer(padding_idx=pad_token_id, n_heads=self.H, dim=self.D, max_length=self.L)\n",
        "        self.att_layer2 = SelfAttentionLayer(padding_idx=pad_token_id, n_heads=self.H, dim=self.D, max_length=self.L)\n",
        "\n",
        "        self.feed_forward = torch.nn.Sequential(\n",
        "            torch.nn.Linear(self.D, self.hidden_size),    # (D, hidden_size)\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(p=0.2),\n",
        "            torch.nn.Linear(self.hidden_size, self.V)     # (hidden_size, V)\n",
        "        )\n",
        "        \n",
        "\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs is a LongTensor of shape (batch_size, max_seq_length)\n",
        "            B: batch_size\n",
        "            L: max_seq_length\n",
        "            D: embedding_dim\n",
        "            V: vocab_size\n",
        "            input shape: (B, L)\n",
        "            pos shape: (B, L)\n",
        "        Returns:\n",
        "            logits of shape (batch_size, max_seq_length, vocab_size)\n",
        "        \"\"\"\n",
        "        B = inputs.shape[0]\n",
        "\n",
        "        att_mask = torch.tril(torch.ones(B, self.L, self.L)).to(device)\n",
        "        att_mask = att_mask.masked_fill(inputs.unsqueeze(1) == self.pad_token_id, 0)\n",
        "        att_mask = att_mask.masked_fill(inputs.unsqueeze(2) == self.pad_token_id, 0)\n",
        "        # input shape: (B, L)\n",
        "        x_emb = self.embeddings_c(inputs) + self.embeddings_p.weight # (B, L, D)\n",
        "        \n",
        "        if debug: print(f'shape x_emb: {x_emb.shape}')                # (B, L, D)\n",
        "\n",
        "        x_emb = self.att_layer1(x_emb, att_mask)     \n",
        "        x_emb = self.att_layer2(x_emb, att_mask)\n",
        "        #logits =  self.att_layer(x_emb, att_mask)                     # (B, L, D)\n",
        "        if debug: print(f'shape x_emb: {x_emb.shape}')              # (B, L, D)\n",
        "        logits = self.feed_forward(x_emb)                            # (B, L, V)\n",
        "        if debug: print(f'shape logits (B, L, V): {logits.shape}')    # (B, L, V)\n",
        "        \n",
        "\n",
        "        return logits"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "else: \n",
        "   dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print('Using {}'.format(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvVgqCENvNfT",
        "outputId": "2754b3e4-1d48-45a8-d6af-38fc7acb80e3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste o modelo com um exemplo"
      ],
      "metadata": {
        "id": "Rm6_PTH2i98e"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwnxfZlrZoT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "051612a7-ccd9-4e35-8152-14f608889129"
      },
      "source": [
        "debug = True\n",
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dim=64,\n",
        "    n_layers=2,\n",
        "    hidden = 128,\n",
        "    n_heads = 4,\n",
        "    pad_token_id=tokenizer.pad_token_id\n",
        ").to(device)\n",
        "\n",
        "\n",
        "sample_input, _ = next(iter(DataLoader(training_dataset)))\n",
        "sample_input = sample_input.to(device)\n",
        "sample_output = model(sample_input)\n",
        "print(f'sample_input.shape: {sample_input.shape}')\n",
        "print(f'sample_output.shape: {sample_output.shape}')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape x_emb: torch.Size([1, 9, 64])\n",
            "scores (B, H, L, L): torch.Size([1, 4, 9, 9])\n",
            "probs (B, H, L, L): torch.Size([1, 4, 9, 9])\n",
            "E (B, H, L, D/H): torch.Size([1, 4, 9, 16])\n",
            "out (B, L, D): torch.Size([1, 9, 64])\n",
            "scores (B, H, L, L): torch.Size([1, 4, 9, 9])\n",
            "probs (B, H, L, L): torch.Size([1, 4, 9, 9])\n",
            "E (B, H, L, D/H): torch.Size([1, 4, 9, 16])\n",
            "out (B, L, D): torch.Size([1, 9, 64])\n",
            "shape x_emb: torch.Size([1, 9, 64])\n",
            "shape logits (B, L, V): torch.Size([1, 9, 29794])\n",
            "sample_input.shape: torch.Size([1, 9])\n",
            "sample_output.shape: torch.Size([1, 9, 29794])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0u4m2eFaeo5",
        "outputId": "5c203aae-8caa-4ae1-f023-9abaa1c7cb01"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  101, 20100,  2308,  3074,  1089,   481,   117,   146,  1189]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3Vh6B-VkA01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba0336b4-073c-4937-9e97-4ecdcce9735f"
      },
      "source": [
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of model parameters: {num_params}')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of model parameters: 5957666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assert da Perplexidade\n"
      ],
      "metadata": {
        "id": "8nhbUVsYnVAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "\n",
        "def perplexity(logits, target, ignore_token_id: int):\n",
        "    \"\"\"\n",
        "    Computes the perplexity.\n",
        "\n",
        "    Args:\n",
        "        logits: a FloatTensor of shape (batch_size, seq_length, vocab_size)\n",
        "        target: a LongTensor of shape (batch_size, seq_length)\n",
        "\n",
        "    Returns:\n",
        "        A float corresponding to the perplexity\n",
        "    \"\"\"\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target = target.reshape(-1)\n",
        "    if debug: \n",
        "      print(f'logits: {logits.shape}')\n",
        "      print(f'train_target_ids: {target.shape}')\n",
        "    loss = nn.functional.cross_entropy(logits, target, reduction='mean', ignore_index=ignore_token_id)\n",
        "    return torch.exp(loss)\n",
        "\n",
        "\n",
        "n_examples = 1000\n",
        "\n",
        "train_input_ids, train_target_ids = next(iter(DataLoader(training_dataset, batch_size=n_examples)))\n",
        "train_input_ids = train_input_ids.to(device)\n",
        "train_target_ids = train_target_ids.to(device)\n",
        "\n",
        "logits = model.forward(train_input_ids)\n",
        "if debug:\n",
        "  print(f'logits: {logits.shape}')\n",
        "  print(f'train_target_ids: {train_target_ids.shape}')\n",
        "\n",
        "my_perplexity = perplexity(logits=logits, target=train_target_ids, ignore_token_id=tokenizer.pad_token_id)\n",
        "if debug:\n",
        "  print(f'correct initial perplexity: {tokenizer.vocab_size}')\n",
        "  print(f'my perplexity:              {int(my_perplexity)}')\n",
        "\n",
        "\n",
        "assert math.isclose(my_perplexity, tokenizer.vocab_size, abs_tol=7000)\n",
        "print('Passou o no assert da perplexidade')\n",
        "run['perplexity'].log(my_perplexity) # Envia perplexity para o Neptune.\n"
      ],
      "metadata": {
        "id": "gbMP8VAUncfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17b4fe2a-d888-4a77-eea4-417af2a147c7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape x_emb: torch.Size([1000, 9, 64])\n",
            "scores (B, H, L, L): torch.Size([1000, 4, 9, 9])\n",
            "probs (B, H, L, L): torch.Size([1000, 4, 9, 9])\n",
            "E (B, H, L, D/H): torch.Size([1000, 4, 9, 16])\n",
            "out (B, L, D): torch.Size([1000, 9, 64])\n",
            "scores (B, H, L, L): torch.Size([1000, 4, 9, 9])\n",
            "probs (B, H, L, L): torch.Size([1000, 4, 9, 9])\n",
            "E (B, H, L, D/H): torch.Size([1000, 4, 9, 16])\n",
            "out (B, L, D): torch.Size([1000, 9, 64])\n",
            "shape x_emb: torch.Size([1000, 9, 64])\n",
            "shape logits (B, L, V): torch.Size([1000, 9, 29794])\n",
            "logits: torch.Size([1000, 9, 29794])\n",
            "train_target_ids: torch.Size([1000, 9])\n",
            "logits: torch.Size([9000, 29794])\n",
            "train_target_ids: torch.Size([9000])\n",
            "correct initial perplexity: 29794\n",
            "my perplexity:              30478\n",
            "Passou o no assert da perplexidade\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YyE_DJkfJ8sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Laço de Treinamento e Validação"
      ],
      "metadata": {
        "id": "KiJtrsqPnE_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = params['batch_size']\n",
        "\n",
        "debug = False\n",
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    max_seq_length=params['max_seq_length'],\n",
        "    dim=params['embedding_dim'],\n",
        "    n_layers=2,\n",
        "    hidden = params['hidden_size'],\n",
        "    n_heads = params['n_heads'],\n",
        "    pad_token_id=tokenizer.pad_token_id\n",
        ").to(device)\n",
        "\n",
        "train_loader = DataLoader(training_dataset, batch_size=params['batch_size'], shuffle=True, drop_last=True)\n",
        "validation_loader = DataLoader(valid_dataset, batch_size=params['batch_size'])\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
        "\n",
        "best_valid_ppl = 10e9\n",
        "\n",
        "if params['retrain']:\n",
        "  model.load_state_dict(torch.load(params['path_saved_model']).state_dict())\n",
        "\n",
        "def train_step(input_ids, target_ids):\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    logits = model(input_ids)\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target_ids = target_ids.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target_ids, ignore_index=model.pad_token_id)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def validation_step(input_ids, target_ids):\n",
        "    model.eval()\n",
        "    logits = model(input_ids)\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target_ids = target_ids.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target_ids, ignore_index=model.pad_token_id)\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "MI7smWjSLY93"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIMSaY-UUGUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bf43f8e-9326-4555-bc92-3a2973942993"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "n_examples = 0\n",
        "step = 0\n",
        "while n_examples < params['max_examples']:\n",
        "    for train_input_ids, train_target_ids in train_loader:\n",
        "      if step < params['last_step']:\n",
        "        step += 1\n",
        "        n_examples += len(train_input_ids)  # Increment of batch size\n",
        "      else:\n",
        "        loss = train_step(train_input_ids.to(device), train_target_ids.to(device)) \n",
        "        train_losses.append(loss)\n",
        "        \n",
        "        if step %  params['eval_every_steps'] == 0:\n",
        "            train_ppl = np.exp(np.average(train_losses))\n",
        "            run['train/ppl'].log(train_ppl) # Envia train ppl para o Neptune.\n",
        "\n",
        "            with torch.no_grad():\n",
        "                valid_ppl = np.exp(np.average([\n",
        "                    validation_step(val_input_ids.to(device), val_target_ids.to(device))\n",
        "                    for val_input_ids, val_target_ids in validation_loader]))\n",
        "            run['valid/ppl'].log(valid_ppl) # Envia valid ppl para o Neptune.\n",
        "            if checkinpoint and valid_ppl < best_valid_ppl:\n",
        "              torch.save(model.state_dict(), 'best_model.pt')\n",
        "              if save_in_drive: torch.save(model, params['path_saved_model'])\n",
        "              print(f\"Best model found in step {step}. valid ppl: {valid_ppl:.2f}, best_valid_ppl: {best_valid_ppl:.2f} \")\n",
        "              best_valid_ppl = valid_ppl\n",
        "            ex_least = n_examples/params['max_examples']*100\n",
        "            print(f'{step} steps; {n_examples} examples so far; {ex_least:.2f} % ; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}, best_valid_ppl: {best_valid_ppl:.2f}')\n",
        "            train_losses = []\n",
        "\n",
        "        n_examples += len(train_input_ids)  # Increment of batch size\n",
        "        step += 1\n",
        "        if n_examples >= params['max_examples']:\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model found in step 280000. valid ppl: 166.30, best_valid_ppl: 10000000000.00 \n",
            "280000 steps; 17920000 examples so far; 11.95 % ; train ppl: 167.30, valid ppl: 166.30, best_valid_ppl: 166.30\n",
            "290000 steps; 18560000 examples so far; 12.37 % ; train ppl: 176.11, valid ppl: 168.51, best_valid_ppl: 166.30\n",
            "300000 steps; 19200000 examples so far; 12.80 % ; train ppl: 175.10, valid ppl: 166.93, best_valid_ppl: 166.30\n",
            "Best model found in step 310000. valid ppl: 165.36, best_valid_ppl: 166.30 \n",
            "310000 steps; 19840000 examples so far; 13.23 % ; train ppl: 173.81, valid ppl: 165.36, best_valid_ppl: 165.36\n",
            "Best model found in step 320000. valid ppl: 164.80, best_valid_ppl: 165.36 \n",
            "320000 steps; 20480000 examples so far; 13.65 % ; train ppl: 172.71, valid ppl: 164.80, best_valid_ppl: 164.80\n",
            "Best model found in step 330000. valid ppl: 164.01, best_valid_ppl: 164.80 \n",
            "330000 steps; 21120000 examples so far; 14.08 % ; train ppl: 172.31, valid ppl: 164.01, best_valid_ppl: 164.01\n",
            "Best model found in step 340000. valid ppl: 162.94, best_valid_ppl: 164.01 \n",
            "340000 steps; 21760000 examples so far; 14.51 % ; train ppl: 171.77, valid ppl: 162.94, best_valid_ppl: 162.94\n",
            "Best model found in step 350000. valid ppl: 162.08, best_valid_ppl: 162.94 \n",
            "350000 steps; 22400000 examples so far; 14.93 % ; train ppl: 170.85, valid ppl: 162.08, best_valid_ppl: 162.08\n",
            "Best model found in step 360000. valid ppl: 161.78, best_valid_ppl: 162.08 \n",
            "360000 steps; 23040000 examples so far; 15.36 % ; train ppl: 168.17, valid ppl: 161.78, best_valid_ppl: 161.78\n",
            "Best model found in step 370000. valid ppl: 160.86, best_valid_ppl: 161.78 \n",
            "370000 steps; 23680000 examples so far; 15.79 % ; train ppl: 168.55, valid ppl: 160.86, best_valid_ppl: 160.86\n",
            "Best model found in step 380000. valid ppl: 160.43, best_valid_ppl: 160.86 \n",
            "380000 steps; 24320000 examples so far; 16.21 % ; train ppl: 168.27, valid ppl: 160.43, best_valid_ppl: 160.43\n",
            "Best model found in step 390000. valid ppl: 159.98, best_valid_ppl: 160.43 \n",
            "390000 steps; 24960000 examples so far; 16.64 % ; train ppl: 167.99, valid ppl: 159.98, best_valid_ppl: 159.98\n",
            "Best model found in step 400000. valid ppl: 159.81, best_valid_ppl: 159.98 \n",
            "400000 steps; 25600000 examples so far; 17.07 % ; train ppl: 167.31, valid ppl: 159.81, best_valid_ppl: 159.81\n",
            "Best model found in step 410000. valid ppl: 159.06, best_valid_ppl: 159.81 \n",
            "410000 steps; 26240000 examples so far; 17.49 % ; train ppl: 167.33, valid ppl: 159.06, best_valid_ppl: 159.06\n",
            "Best model found in step 420000. valid ppl: 158.59, best_valid_ppl: 159.06 \n",
            "420000 steps; 26880000 examples so far; 17.92 % ; train ppl: 166.45, valid ppl: 158.59, best_valid_ppl: 158.59\n",
            "Best model found in step 430000. valid ppl: 157.84, best_valid_ppl: 158.59 \n",
            "430000 steps; 27520000 examples so far; 18.35 % ; train ppl: 166.14, valid ppl: 157.84, best_valid_ppl: 157.84\n",
            "Best model found in step 440000. valid ppl: 157.62, best_valid_ppl: 157.84 \n",
            "440000 steps; 28160000 examples so far; 18.77 % ; train ppl: 165.11, valid ppl: 157.62, best_valid_ppl: 157.62\n",
            "Best model found in step 450000. valid ppl: 157.27, best_valid_ppl: 157.62 \n",
            "450000 steps; 28800000 examples so far; 19.20 % ; train ppl: 164.84, valid ppl: 157.27, best_valid_ppl: 157.27\n",
            "Best model found in step 460000. valid ppl: 156.69, best_valid_ppl: 157.27 \n",
            "460000 steps; 29440000 examples so far; 19.63 % ; train ppl: 164.56, valid ppl: 156.69, best_valid_ppl: 156.69\n",
            "Best model found in step 470000. valid ppl: 155.97, best_valid_ppl: 156.69 \n",
            "470000 steps; 30080000 examples so far; 20.05 % ; train ppl: 164.18, valid ppl: 155.97, best_valid_ppl: 155.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliação final no dataset de teste\n",
        "\n",
        "\n",
        "Bonus: o modelo com menor perplexidade no dataset de testes ganhará 0.5 ponto na nota final."
      ],
      "metadata": {
        "id": "VgdNymJdNPXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if params['retrain']:\n",
        "  model.load_state_dict(torch.load(params['path_saved_model']).state_dict())"
      ],
      "metadata": {
        "id": "wKxF1aykIYdO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxN5YytzZ7Tn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e94737e6-8457-43bb-f60a-53ad36be367f"
      },
      "source": [
        "#best_model = 'best_model.pt'\n",
        "\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=params['batch_size'])\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_ppl = np.exp(np.average([\n",
        "        validation_step(test_input_ids.to(device), test_target_ids.to(device))\n",
        "        for test_input_ids, test_target_ids in test_loader\n",
        "    ]))\n",
        "\n",
        "print(f'test perplexity: {test_ppl}')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test perplexity: 154.7757638150705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run['test/perplexity'].log(test_ppl)\n"
      ],
      "metadata": {
        "id": "dqKk8eGXKgfO"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run.stop()"
      ],
      "metadata": {
        "id": "ZLT8x2WYKicU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27a51bbb-00d8-4bb1-a773-9d1c76910124"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 4 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 4 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/leolellisr/dl-ia025/e/DLIA-126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste seu modelo com uma sentença\n",
        "\n",
        "Escolha uma sentença gerada pelo modelo que ache interessante."
      ],
      "metadata": {
        "id": "BHvEs8mPszy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Eu gosto de comer pizza pois me faz'\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    print(f'pred: {predicted_id}')\n",
        "    print(f'input_ids: {input_ids}')\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "pqxFR-wRvflc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "935758e5-70ab-4277-c2e9-d859132771e5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pred: 13519\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519]\n",
            "Eu gosto de comer pizza pois me faz lembrar\n",
            "pred: 179\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 179]\n",
            "Eu gosto de comer pizza pois me faz lembrar que\n",
            "pred: 146\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 179, 146]\n",
            "Eu gosto de comer pizza pois me faz lembrar que o\n",
            "pred: 179\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 179, 146, 179]\n",
            "Eu gosto de comer pizza pois me faz lembrar que o que\n",
            "pred: 2779\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 179, 146, 179, 2779]\n",
            "Eu gosto de comer pizza pois me faz lembrar que o que eu\n",
            "pred: 346\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 179, 146, 179, 2779, 346]\n",
            "Eu gosto de comer pizza pois me faz lembrar que o que eu não\n",
            "pred: 311\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 179, 146, 179, 2779, 346, 311]\n",
            "Eu gosto de comer pizza pois me faz lembrar que o que eu não me\n",
            "pred: 5069\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 179, 146, 179, 2779, 346, 311, 5069]\n",
            "Eu gosto de comer pizza pois me faz lembrar que o que eu não me lemb\n",
            "pred: 157\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 179, 146, 179, 2779, 346, 311, 5069, 157]\n",
            "Eu gosto de comer pizza pois me faz lembrar que o que eu não me lembro\n",
            "pred: 125\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 179, 146, 179, 2779, 346, 311, 5069, 157, 125]\n",
            "Eu gosto de comer pizza pois me faz lembrar que o que eu não me lembro de\n",
            "pred: 179\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 179, 146, 179, 2779, 346, 311, 5069, 157, 125, 179]\n",
            "Eu gosto de comer pizza pois me faz lembrar que o que eu não me lembro de que\n",
            "pred: 146\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 179, 146, 179, 2779, 346, 311, 5069, 157, 125, 179, 146]\n",
            "Eu gosto de comer pizza pois me faz lembrar que o que eu não me lembro de que o\n",
            "pred: 179\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 179, 146, 179, 2779, 346, 311, 5069, 157, 125, 179, 146, 179]\n",
            "Eu gosto de comer pizza pois me faz lembrar que o que eu não me lembro de que o que\n",
            "pred: 2779\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 179, 146, 179, 2779, 346, 311, 5069, 157, 125, 179, 146, 179, 2779]\n",
            "Eu gosto de comer pizza pois me faz lembrar que o que eu não me lembro de que o que eu\n",
            "pred: 346\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 179, 146, 179, 2779, 346, 311, 5069, 157, 125, 179, 146, 179, 2779, 346]\n",
            "Eu gosto de comer pizza pois me faz lembrar que o que eu não me lembro de que o que eu não\n",
            "pred: 311\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 179, 146, 179, 2779, 346, 311, 5069, 157, 125, 179, 146, 179, 2779, 346, 311]\n",
            "Eu gosto de comer pizza pois me faz lembrar que o que eu não me lembro de que o que eu não me\n",
            "pred: 5069\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 179, 146, 179, 2779, 346, 311, 5069, 157, 125, 179, 146, 179, 2779, 346, 311, 5069]\n",
            "Eu gosto de comer pizza pois me faz lembrar que o que eu não me lembro de que o que eu não me lemb\n",
            "pred: 157\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 179, 146, 179, 2779, 346, 311, 5069, 157, 125, 179, 146, 179, 2779, 346, 311, 5069, 157]\n",
            "Eu gosto de comer pizza pois me faz lembrar que o que eu não me lembro de que o que eu não me lembro\n",
            "pred: 125\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 179, 146, 179, 2779, 346, 311, 5069, 157, 125, 179, 146, 179, 2779, 346, 311, 5069, 157, 125]\n",
            "Eu gosto de comer pizza pois me faz lembrar que o que eu não me lembro de que o que eu não me lembro de\n",
            "pred: 179\n",
            "input_ids: [3396, 10303, 125, 1847, 13779, 15616, 1502, 311, 659, 13519, 179, 146, 179, 2779, 346, 311, 5069, 157, 125, 179, 146, 179, 2779, 346, 311, 5069, 157, 125, 179]\n",
            "Eu gosto de comer pizza pois me faz lembrar que o que eu não me lembro de que o que eu não me lembro de que\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Ouviram do Ipiranga em suas margens plácidas um grito'\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "bzU6DHWHvhDT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78152e29-73f6-40b6-9ddb-bbdf6503dbdb"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ouviram do Ipiranga em suas margens plácidas um grito de\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de um\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de um lado\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de um lado,\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de um lado, que\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de um lado, que é\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de um lado, que é o\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de um lado, que é o que\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de um lado, que é o que é\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de um lado, que é o que é o\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de um lado, que é o que é o que\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de um lado, que é o que é o que é\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de um lado, que é o que é o que é o\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de um lado, que é o que é o que é o que\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de um lado, que é o que é o que é o que é\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de um lado, que é o que é o que é o que é o\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de um lado, que é o que é o que é o que é o que\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de um lado, que é o que é o que é o que é o que é\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de um lado, que é o que é o que é o que é o que é o\n",
            "Ouviram do Ipiranga em suas margens plácidas um grito de um lado, que é o que é o que é o que é o que é o que\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt ='A galinha atravessou a rua para chegar'\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "i8NDCTAmyf25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89d7d5d7-7470-4ca5-d576-583ec0bd9e5b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A galinha atravessou a rua para chegar a\n",
            "A galinha atravessou a rua para chegar a um\n",
            "A galinha atravessou a rua para chegar a um dia\n",
            "A galinha atravessou a rua para chegar a um dia,\n",
            "A galinha atravessou a rua para chegar a um dia, mas\n",
            "A galinha atravessou a rua para chegar a um dia, mas não\n",
            "A galinha atravessou a rua para chegar a um dia, mas não é\n",
            "A galinha atravessou a rua para chegar a um dia, mas não é a\n",
            "A galinha atravessou a rua para chegar a um dia, mas não é a primeira\n",
            "A galinha atravessou a rua para chegar a um dia, mas não é a primeira vez\n",
            "A galinha atravessou a rua para chegar a um dia, mas não é a primeira vez que\n",
            "A galinha atravessou a rua para chegar a um dia, mas não é a primeira vez que o\n",
            "A galinha atravessou a rua para chegar a um dia, mas não é a primeira vez que o que\n",
            "A galinha atravessou a rua para chegar a um dia, mas não é a primeira vez que o que é\n",
            "A galinha atravessou a rua para chegar a um dia, mas não é a primeira vez que o que é o\n",
            "A galinha atravessou a rua para chegar a um dia, mas não é a primeira vez que o que é o que\n",
            "A galinha atravessou a rua para chegar a um dia, mas não é a primeira vez que o que é o que é\n",
            "A galinha atravessou a rua para chegar a um dia, mas não é a primeira vez que o que é o que é o\n",
            "A galinha atravessou a rua para chegar a um dia, mas não é a primeira vez que o que é o que é o que\n",
            "A galinha atravessou a rua para chegar a um dia, mas não é a primeira vez que o que é o que é o que é\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt ='Ouça com cuidado, o segredo para a felicidade é'\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "XOefflDDynTw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d671116-7e10-4637-b802-0b75619cf322"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ouça com cuidado, o segredo para a felicidade é que\n",
            "Ouça com cuidado, o segredo para a felicidade é que o\n",
            "Ouça com cuidado, o segredo para a felicidade é que o que\n",
            "Ouça com cuidado, o segredo para a felicidade é que o que é\n",
            "Ouça com cuidado, o segredo para a felicidade é que o que é o\n",
            "Ouça com cuidado, o segredo para a felicidade é que o que é o que\n",
            "Ouça com cuidado, o segredo para a felicidade é que o que é o que é\n",
            "Ouça com cuidado, o segredo para a felicidade é que o que é o que é o\n",
            "Ouça com cuidado, o segredo para a felicidade é que o que é o que é o que\n",
            "Ouça com cuidado, o segredo para a felicidade é que o que é o que é o que é\n",
            "Ouça com cuidado, o segredo para a felicidade é que o que é o que é o que é o\n",
            "Ouça com cuidado, o segredo para a felicidade é que o que é o que é o que é o que\n",
            "Ouça com cuidado, o segredo para a felicidade é que o que é o que é o que é o que é\n",
            "Ouça com cuidado, o segredo para a felicidade é que o que é o que é o que é o que é o\n",
            "Ouça com cuidado, o segredo para a felicidade é que o que é o que é o que é o que é o que\n",
            "Ouça com cuidado, o segredo para a felicidade é que o que é o que é o que é o que é o que é\n",
            "Ouça com cuidado, o segredo para a felicidade é que o que é o que é o que é o que é o que é o\n",
            "Ouça com cuidado, o segredo para a felicidade é que o que é o que é o que é o que é o que é o que\n",
            "Ouça com cuidado, o segredo para a felicidade é que o que é o que é o que é o que é o que é o que é\n",
            "Ouça com cuidado, o segredo para a felicidade é que o que é o que é o que é o que é o que é o que é o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt ='Temos que pegar! Isso eu sei. Pegá-los eu tentarei! Vai ser grande a'\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "mJJHktsjyIfp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f5aae1a-7813-43b7-b589-972a8dc9a4e8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a primeira\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a primeira vez\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a primeira vez que\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a primeira vez que o\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a primeira vez que o que\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a primeira vez que o que é\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a primeira vez que o que é o\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a primeira vez que o que é o que\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a primeira vez que o que é o que é\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a primeira vez que o que é o que é o\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a primeira vez que o que é o que é o que\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a primeira vez que o que é o que é o que é\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a primeira vez que o que é o que é o que é o\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a primeira vez que o que é o que é o que é o que\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a primeira vez que o que é o que é o que é o que é\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a primeira vez que o que é o que é o que é o que é o\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a primeira vez que o que é o que é o que é o que é o que\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a primeira vez que o que é o que é o que é o que é o que é\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a primeira vez que o que é o que é o que é o que é o que é o\n",
            "Temos que pegar! Isso eu sei. Pegá - los eu tentarei! Vai ser grande a primeira vez que o que é o que é o que é o que é o que é o que\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt ='Desejo a todas as inimigas vida longa'\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "AT_dpOD8y2_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b64f82e-cb53-4760-bb06-23894c919b82"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Desejo a todas as inimigas vida longa,\n",
            "Desejo a todas as inimigas vida longa, mas\n",
            "Desejo a todas as inimigas vida longa, mas não\n",
            "Desejo a todas as inimigas vida longa, mas não é\n",
            "Desejo a todas as inimigas vida longa, mas não é a\n",
            "Desejo a todas as inimigas vida longa, mas não é a primeira\n",
            "Desejo a todas as inimigas vida longa, mas não é a primeira vez\n",
            "Desejo a todas as inimigas vida longa, mas não é a primeira vez que\n",
            "Desejo a todas as inimigas vida longa, mas não é a primeira vez que o\n",
            "Desejo a todas as inimigas vida longa, mas não é a primeira vez que o que\n",
            "Desejo a todas as inimigas vida longa, mas não é a primeira vez que o que é\n",
            "Desejo a todas as inimigas vida longa, mas não é a primeira vez que o que é o\n",
            "Desejo a todas as inimigas vida longa, mas não é a primeira vez que o que é o que\n",
            "Desejo a todas as inimigas vida longa, mas não é a primeira vez que o que é o que é\n",
            "Desejo a todas as inimigas vida longa, mas não é a primeira vez que o que é o que é o\n",
            "Desejo a todas as inimigas vida longa, mas não é a primeira vez que o que é o que é o que\n",
            "Desejo a todas as inimigas vida longa, mas não é a primeira vez que o que é o que é o que é\n",
            "Desejo a todas as inimigas vida longa, mas não é a primeira vez que o que é o que é o que é o\n",
            "Desejo a todas as inimigas vida longa, mas não é a primeira vez que o que é o que é o que é o que\n",
            "Desejo a todas as inimigas vida longa, mas não é a primeira vez que o que é o que é o que é o que é\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = next(iter(test_loader))\n",
        "input_ids = test[0][1]\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "input_ids = input_ids.tolist()\n",
        "prompt = tokenizer.decode(input_ids)\n",
        "print(prompt)\n",
        "for _ in range(max_output_tokens):\n",
        "    #input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    #print(input_ids)\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    \n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    #print(input_ids)\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "lpvUtaRLzJHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9113da3-ecb8-4a2c-b3d2-3ad46bb3d8a0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##cio, chega ao Brasil numa tradução completa e\n",
            "##cio, chega ao Brasil numa tradução completa e a\n",
            "##cio, chega ao Brasil numa tradução completa e a sua\n",
            "##cio, chega ao Brasil numa tradução completa e a sua vida\n",
            "##cio, chega ao Brasil numa tradução completa e a sua vida.\n",
            "##cio, chega ao Brasil numa tradução completa e a sua vida. O\n",
            "##cio, chega ao Brasil numa tradução completa e a sua vida. O que\n",
            "##cio, chega ao Brasil numa tradução completa e a sua vida. O que é\n",
            "##cio, chega ao Brasil numa tradução completa e a sua vida. O que é que\n",
            "##cio, chega ao Brasil numa tradução completa e a sua vida. O que é que o\n",
            "##cio, chega ao Brasil numa tradução completa e a sua vida. O que é que o que\n",
            "##cio, chega ao Brasil numa tradução completa e a sua vida. O que é que o que é\n",
            "##cio, chega ao Brasil numa tradução completa e a sua vida. O que é que o que é o\n",
            "##cio, chega ao Brasil numa tradução completa e a sua vida. O que é que o que é o que\n",
            "##cio, chega ao Brasil numa tradução completa e a sua vida. O que é que o que é o que é\n",
            "##cio, chega ao Brasil numa tradução completa e a sua vida. O que é que o que é o que é o\n",
            "##cio, chega ao Brasil numa tradução completa e a sua vida. O que é que o que é o que é o que\n",
            "##cio, chega ao Brasil numa tradução completa e a sua vida. O que é que o que é o que é o que é\n",
            "##cio, chega ao Brasil numa tradução completa e a sua vida. O que é que o que é o que é o que é o\n",
            "##cio, chega ao Brasil numa tradução completa e a sua vida. O que é que o que é o que é o que é o que\n",
            "##cio, chega ao Brasil numa tradução completa e a sua vida. O que é que o que é o que é o que é o que é\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = next(iter(train_loader))\n",
        "input_ids = test[0][1]\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "input_ids = input_ids.tolist()\n",
        "prompt = tokenizer.decode(input_ids)\n",
        "print(prompt)\n",
        "for _ in range(max_output_tokens):\n",
        "    #input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    #print(input_ids)\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    \n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    #print(input_ids)\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "trv2e9fR1eb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "112ae6e1-d8ac-4bc0-ea57-2454051cf137"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "expressões políticas, culturais e históricas particulares, localizadas\n",
            "expressões políticas, culturais e históricas particulares, localizadas em\n",
            "expressões políticas, culturais e históricas particulares, localizadas em um\n",
            "expressões políticas, culturais e históricas particulares, localizadas em um país\n",
            "expressões políticas, culturais e históricas particulares, localizadas em um país,\n",
            "expressões políticas, culturais e históricas particulares, localizadas em um país, que\n",
            "expressões políticas, culturais e históricas particulares, localizadas em um país, que não\n",
            "expressões políticas, culturais e históricas particulares, localizadas em um país, que não se\n",
            "expressões políticas, culturais e históricas particulares, localizadas em um país, que não se pode\n",
            "expressões políticas, culturais e históricas particulares, localizadas em um país, que não se pode fazer\n",
            "expressões políticas, culturais e históricas particulares, localizadas em um país, que não se pode fazer.\n",
            "expressões políticas, culturais e históricas particulares, localizadas em um país, que não se pode fazer. O\n",
            "expressões políticas, culturais e históricas particulares, localizadas em um país, que não se pode fazer. O que\n",
            "expressões políticas, culturais e históricas particulares, localizadas em um país, que não se pode fazer. O que é\n",
            "expressões políticas, culturais e históricas particulares, localizadas em um país, que não se pode fazer. O que é que\n",
            "expressões políticas, culturais e históricas particulares, localizadas em um país, que não se pode fazer. O que é que o\n",
            "expressões políticas, culturais e históricas particulares, localizadas em um país, que não se pode fazer. O que é que o que\n",
            "expressões políticas, culturais e históricas particulares, localizadas em um país, que não se pode fazer. O que é que o que é\n",
            "expressões políticas, culturais e históricas particulares, localizadas em um país, que não se pode fazer. O que é que o que é o\n",
            "expressões políticas, culturais e históricas particulares, localizadas em um país, que não se pode fazer. O que é que o que é o que\n",
            "expressões políticas, culturais e históricas particulares, localizadas em um país, que não se pode fazer. O que é que o que é o que é\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = next(iter(validation_loader))\n",
        "input_ids = test[0][1]\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "input_ids = input_ids.tolist()\n",
        "prompt = tokenizer.decode(input_ids)\n",
        "print(prompt)\n",
        "for _ in range(max_output_tokens):\n",
        "    #input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    #print(input_ids)\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    \n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    #print(input_ids)\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "4-t5IeyD1jdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8cb6018-3ad1-4cc8-a6bd-ecdd0f4039e2"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "palestrante. Trabalhou como redator e diretor\n",
            "palestrante. Trabalhou como redator e diretor de\n",
            "palestrante. Trabalhou como redator e diretor de Educação\n",
            "palestrante. Trabalhou como redator e diretor de Educação,\n",
            "palestrante. Trabalhou como redator e diretor de Educação, que\n",
            "palestrante. Trabalhou como redator e diretor de Educação, que é\n",
            "palestrante. Trabalhou como redator e diretor de Educação, que é o\n",
            "palestrante. Trabalhou como redator e diretor de Educação, que é o que\n",
            "palestrante. Trabalhou como redator e diretor de Educação, que é o que é\n",
            "palestrante. Trabalhou como redator e diretor de Educação, que é o que é o\n",
            "palestrante. Trabalhou como redator e diretor de Educação, que é o que é o que\n",
            "palestrante. Trabalhou como redator e diretor de Educação, que é o que é o que é\n",
            "palestrante. Trabalhou como redator e diretor de Educação, que é o que é o que é o\n",
            "palestrante. Trabalhou como redator e diretor de Educação, que é o que é o que é o que\n",
            "palestrante. Trabalhou como redator e diretor de Educação, que é o que é o que é o que é\n",
            "palestrante. Trabalhou como redator e diretor de Educação, que é o que é o que é o que é o\n",
            "palestrante. Trabalhou como redator e diretor de Educação, que é o que é o que é o que é o que\n",
            "palestrante. Trabalhou como redator e diretor de Educação, que é o que é o que é o que é o que é\n",
            "palestrante. Trabalhou como redator e diretor de Educação, que é o que é o que é o que é o que é o\n",
            "palestrante. Trabalhou como redator e diretor de Educação, que é o que é o que é o que é o que é o que\n",
            "palestrante. Trabalhou como redator e diretor de Educação, que é o que é o que é o que é o que é o que é\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus 1\n",
        "Quem conseguir a menor perplexidade no dataset de testes ganha 0.5 ponto na média final.\n",
        "\n",
        "## Bonus 2\n",
        "Qual é a complexidade (em notação O-grande) da função de geração de texto acima?\n",
        "\n",
        "Quem responder corretamente a pergunta acima e deixar a função com menor complexidade ganha 0.5 ponto na média final."
      ],
      "metadata": {
        "id": "nGdxlXhGq7Ua"
      }
    }
  ]
}