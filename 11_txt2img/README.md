# Text-to-Image generation with diffusion models

## Proposal
This work proposes implementations of the base 64×64 text-to-image diffusion model of [Imagen](https://arxiv.org/pdf/2205.11487.pdf) (adaptation of U-Net architecture from [Nichol and Dhariwal (2021)](https://arxiv.org/abs/2102.09672) - [git](https://github.com/openai/improved-diffusion)) with fewer parameters, in order to demonstrate the use of diffusion models in the task of generating images from text on machines with limited memory and without access to multiple GPUs.

## Methodology
[COCO Dataset](https://cocodataset.org) is used for training, validation, and testing. The images are used as input for the diffusion model with the embeddings and attentional masks generated by a T5 encoder model from the image captions. The model is trained based on the perception prioritized (P2) loss, proposed by [Choi et. al (2021)](https://arxiv.org/pdf/2204.00227.pdf). The model is also validated with the [Fréchet Inception Distance (FID)](https://arxiv.org/pdf/1706.08500.pdf). The model training, validation, and testing flowchart is shown [here](https://imgur.com/24inAbh).

## Experiments

This work experiments have been done by feeding text-to-image diffusion models with images (which has Gaussian noise added through the forward diffusion Process) and with embeddings and attention masks collected from annotations with a T5 Encoder model, with the objective of minimizing the prioritized perception (P2) loss and validating the images generated by these models also with Fréchet Inception Distance (FID).

Considering the limitations of the machine used, different versions of the text-to-image model were implemented, with different parameters, with a view to optimizing the training and improving the performance of models. A comparison between the different parameters and est results of each version of the implemented model and the data available in the Imagen paper is presented [here](https://imgur.com/ptt0DSe).

The following versions of this work's text-to-image model have been implemented:
- small: version used to test the model architecture;
- base: version used to test overfit in one batch;
- large: version used for parameter optimization between base and full version;
- full: version with some Imagen parameters (with limitations).

## Conclusion
It was possible to demonstrate the use of diffusion models to generate images from text. The images generated by the base and large models match in shapes and colors with the available captions.

In the study of the text conditioning scale, it was noticed that larger scales (up to 10) collaborated for better Perception Prioritized (P2) Loss and Fréchet Inception Distance (FID) results. The images generated by the model that used a text conditioning scale of 8 also matched the input texts better.

In the analysis of the validation metrics to save the model during the training and optimization of the large model, it was found that the use of only Perception Prioritized (P2) Loss produces better results for Perception Prioritized (P2) Loss and Fréchet Inception Distance (FID) in less training time. However, the use of Perception Prioritized (P2) Loss and Fréchet Inception Distance (FID) promoted better-generated images.

It was also possible to demonstrate the feasibility of using diffusion models in the task of generating images from text, through the overfit in a batch. Finally, it was possible to generate a comparison between the validation with Perception Prioritized (P2) Loss and with Perception Prioritized (P2) Loss and Fréchet Inception Distance (FID).

## Links

- Graphs and images available Neptune: https://app.neptune.ai/leolellisr/dl-ia025/;
- Saved models: https://drive.google.com/drive/folders/15eIsUCaQS-sAHxsbwbB9OU6bSPVZ_g85?usp=sharing
- ppt presentation: https://docs.google.com/presentation/d/182YQUwR82r37zeTlkq9fBs0v3ADj0QdJSqjmIH9NWs4/edit?usp=sharing
- paper pdf: https://github.com/leolellisr/deep_learning_projects/blob/main/11_txt2img/paper_Final_Project_DL_22.pdf
- paper Overleaf: https://www.overleaf.com/read/rrsmrbscmnxf
